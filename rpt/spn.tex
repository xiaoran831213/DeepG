\documentclass[11pt]{book}
\usepackage{textcomp,bbding,subfig}
\usepackage{float,amssymb,amsmath,amsfonts,bm}
\usepackage{graphicx,cite}
\usepackage[]{natbib}
\def\style{apa}
\usepackage[usenames,pdftex,dvips]{color,xcolor}
\usepackage{multirow,tabulary,colortbl,array}
\usepackage[normalem]{ulem}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{moreverb,setspace}
%
% Text layout
\topmargin -1.5cm
\oddsidemargin 0.0cm
\evensidemargin 0.0cm
\textwidth 16.5cm
\textheight 23.5cm     
%
% Remove brackets from numbering in List of References
% \makeatletter \renewcommand\@biblabel[1]{} \makeatother
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother
%
% no indent for any paragraphs
\setlength\parindent{0pt}
%
\numberwithin{equation}{chapter}
%
% aliasis
% FreeSurfer from Havord Unv.
\newcommand{\bs}{\boldsymbol}
\newcommand{\mean}[2]{\left\langle{#1}\right\rangle_{#2}}
% vector, matrices
\newcommand{\vs}{\bs{s}}
\newcommand{\vst}{\bs{\tilde{s}}}
% derivative
\newcommand{\DRV}[2]{\frac{d #1}{d #2}}
\newcommand{\DRC}[3]{\DRV{#1}{#2}\DRV{#2}{#3}}
\newcommand{\PDV}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\PDC}[3]{\PDV{#1}{#2}\PDV{#2}{#3}}
%
\doublespacing
\begin{document}
\title{A note on 'An Introduction to the Theory of Spin Glasses and Neural Networks'}
\maketitle
\begin{flushleft}
Xiaoran Tong\textsuperscript{1},
\\
\bigskip
\textbf{1} Department of Epidemiology and Biostatistics, Michigan State University, East Lansing, USA
%
\vskip 50ex
Correspondence: Qing Lu\\
Department of Epidemiology and Biostatistics\\
College of Human Medicine\\
Michigan State University\\
909 Fee Road\\
East Lansing, MI 48824--1030\\
\end{flushleft}
\clearpage
\chapter{The Ising Magnetic Systems}
\section{General principles of the statistical mechanics}
\newcommand{\hf}{H}
\newcommand{\ef}{E}
\newcommand{\pf}{P}
\newcommand{\ps}{P(\vs)}
\newcommand{\pst}{\pf(\vst)}
\newcommand{\es}{\ef(\vs)}
\newcommand{\est}{\ef(\vst)}
\newcommand{\me}{\mathcal{\bs{\ef}}}
The state of a system is represented by $N$-d variable $\vs=[s_1, s_2, \dots, s_N]^T$. An observable quantity of the system is a function of its state $A(\vs)$. The form of $A$ is versatile, but the most basic one is called \textit{engery} $\ef = \ef(\vs)$ that characterize each state, and further governs the dynamic behavior of the system.

In most cases the mean of $A$ holds better interests than $A(\vs)$ at particular moments, which can be formally obtained via a infinitely long peroid of observation:
\begin{equation} \label{eq:mu(A,t)}
  \mean{A}{t} = \lim_{t \to \infty}\frac{1}{t} \int_0^t A(\vs^t) dt
\end{equation}
The equilibrium statistical mechanics states that, after a infinitely long period of observation, the system has ``visited'' its distinct states many times, and therefore the mean over time in Eq.\,\eqref{eq:mu(A,t)} can be replaced by the mean over \textit{assemble} of the states:
\begin{align}\label{eq:mu(A,s)}
  \mean{A}{\vs} = \int_{\vs} A(\vs)\Pr(\vs) d\vs.
\end{align}
Here, $\Pr(\vs)$ is the \textit{probability density function} (PDF) of the sytem states, which must satisfies
\begin{align}\label{eq:int(p)=1}
  \int_{\vs} \Pr(\vs) d\vs = 1
\end{align}
in order to be a valid PDF.

To characterize the distribution of states $\pf = \ps$, statistical mechanics introduces the concept of entropy, defined by the mean negative logarithm of the density itself:
\begin{align}
  \hf = \mean{-\log{\ps}}{\vs} = -\int{\ps\log{\ps}\,d\vs}.
\end{align}
Entropy is non-negative, it measures how chaotic the system is. In general, the flatter the distribution $P$ is, the larger the entroy $\hf$ become (i.e.\, so disordered that many states have similar chance to occue); on the contrary, a point mass distribution $\delta(\vs_0)$ has 0 entropy (i.e.\, extremely ordered that a single state $\vs_0$ dominants the system).

\textbf{work out the general form of $\pf = \ps$} \\
The law of nature dictates that, without external interference (e.g., chane of temperature or pressure),
\begin{itemize}
\item the mean system ergery
\begin{align} \label{eq:e=mu(h)}
  \me = \mean{\ef}{\vs} = \int_{\vs} \es \ps d\vs
\end{align} is conserved;
\item when the system achieves equilibrium, it is also at the most disordered dynamics as possible, without breaking the previous rule \eqref{eq:e=mu(h)};
\end{itemize}
in other words, the choice of state distribution $\pf$ must maximize the entropy $\hf$ under the rule of energy conservation \eqref{eq:e=mu(h)} and probability normalization \eqref{eq:int(p)=1}, collectively expressed by the following optimization objective
\begin{align}\label{eq:max(e, p)}
  L(\ps, \lambda, \gamma) = -\int_{\vs}\ps\log{\ps}\,d\vs - \beta(\int_{\vs} {\es\ps\,d\vs} - \me) - \gamma(\int_{\vs}{\ps\,d\vs} - 1)
\end{align}
where $\beta$, $\gamma$ are Lagrange multipliers, and $L(\ps; \beta, \gamma)$ is to be maximized. The derivative of $L$ w.r.t. $\{\ps, \beta, \gamma\}$ is
\begin{equation*}
  \begin{cases}
    \PDV{L}{\ps}      & = -log{\ps} - 1 - \beta\es -\gamma \\
    \PDV{L}{\beta}    & = -\int_{\vst} \left[\est\pst + 1\right] d\vst + \me\\
    \PDV{L}{\gamma}   & = -\int_{\vst} \pst  d\vst + 1
  \end{cases}
\end{equation*}
\clearpage
\singlespacing
\bibliographystyle{\style}
\bibliography{ref}
% \printbibliography{}
%
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
\grid
